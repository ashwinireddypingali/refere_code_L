{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\">---:Tensorflow:--</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3><strong>In this notebook we will overview the basics of TensorFlow, learn it's structures and see what is the motivation to use it</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. This flexible architecture lets you deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device without rewriting code. TensorFlow also includes TensorBoard, a data visualization toolkit.\n",
    "\n",
    "TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"ref2\"></a>\n",
    "# How does TensorFlow work?\n",
    "\n",
    "TensorFlow's capability to execute the code on different devices such as CPUs and GPUs is a consequence of it's specific structure:\n",
    "\n",
    "TensorFlow defines computations as Graphs, and these are made with operations (also know as “ops”). So, when we work with TensorFlow, it is the same as defining a series of operations in a Graph.\n",
    "\n",
    "To execute these operations as computations, we must launch the Graph into a Session. The session translates and passes the operations represented into the graphs to the device you want to execute them on, be it a GPU or CPU.\n",
    "\n",
    "For example, the image below represents a graph in TensorFlow. _W_, _x_ and b are tensors over the edges of this graph. _MatMul_ is an operation over the tensors _W_ and _x_, after that _Add_ is called and add the result of the previous operator with _b_. The resultant tensors of each operation cross the next one until the end where it's possible to get the wanted result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "*constants*:\n",
    "    \n",
    "    Ex: a = tf.constant([2])\n",
    "\n",
    "\n",
    "*PlaceHolder* :\n",
    "\n",
    "        If you want to feed data to a TensorFlow model from outside a model, you will need to use placeholders.\n",
    "        \n",
    "    ex: a=tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "*Variables*:\n",
    "\n",
    "        To define variables we use the command tf.variable(). To be able to use variables in a computation graph it is necessary to initialize them before running the graph in a session. \n",
    "        This is done by running tf.global_variables_initializer().\n",
    "    To update the value of a variable, we simply run an assign operation that assigns a value to the variable:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Building a  Computaional graph and Launching session:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TensorFlow\n",
    "<p>To use TensorFlow, we need to import the library. We imported it and optionally gave it the name \"tf\", so the modules can be accessed by __tf.module-name__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "node1 = tf.constant(3.0,tf.float32) #These are fixed values, we couldn't change.\n",
    "node2 = tf.constant(4.0)\n",
    "sess = tf.Session() #Initialization of session to run the created computational graph.\n",
    "print(sess.run([node1,node2])) #Here closed using close() funciton\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as s: ##Automatically closed\n",
    "    print(s.run([node1,node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the meaning of Tensor?\n",
    "\n",
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>In TensorFlow all data is passed between operations in a computation graph, and these are passed in the form of Tensors, hence the name of TensorFlow.</strong></font>\n",
    "<br>\n",
    "<br>\n",
    "The word __tensor__ from new latin means \"that which stretches\". It is a mathematical object that is named __tensor__  because an early application of tensors was the study of materials stretching under tension. The contemporary meaning of tensors can be taken as multidimensional arrays. \n",
    "\n",
    "<p></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "That's great, but... what are these multidimensional arrays? \n",
    "\n",
    "Going back a little bit to physics to understand the concept of dimensions:<br>\n",
    "<img src=\"https://ibm.box.com/shared/static/ymn0hl3hf8s3xb4k15v22y5vmuodnue1.svg\"/>\n",
    "<div style=\"text-align:center\">[[Image Source]](https://en.wikipedia.org/wiki/Dimension) </div>\n",
    "\n",
    "The zero dimension can be seen as a point, a single object or a single item.\n",
    "\n",
    "The first dimension can be seen as a line, a one-dimensional array can be seen as numbers along this line, or as points along the line. One dimension can contain infinite zero dimension/points elements.\n",
    "\n",
    "The second dimension can be seen as a surface, a two-dimensional array can be seen as an infinite series of lines along an infinite line. \n",
    "\n",
    "The third dimension can be seen as volume, a three-dimensional array can be seen as an infinite series of surfaces along an infinite line.\n",
    "\n",
    "The Fourth dimension can be seen as the hyperspace or spacetime, a volume varying through time, or an infinite series of volumes along an infinite line. And so forth on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Tensors?\n",
    "\n",
    "The Tensor structure helps us by giving the freedom to shape the dataset the way we want.\n",
    "\n",
    "And it is particularly helpful when dealing with images, due to the nature of how information in images are encoded,\n",
    "\n",
    "Thinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional strucutre (a matrix)... until you remember that images have colors, and to add information about the colors, we need another dimension, and thats when Tensors become particulary helpful.\n",
    "\n",
    "Images are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity of each channel color into the width and height of the image, just like this:\n",
    "\n",
    "<img src='https://ibm.box.com/shared/static/xlpv9h5xws248c09k1rlx7cer69y4grh.png'>\n",
    "<div style=\"text-align:center\">[[Image Source]](https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131.aspx)</div>\n",
    "\n",
    "So the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing \"Multiplication computation\" using TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without using inbuilt function output is:\n",
      " 30\n",
      "With using inbuilt function output is:\n",
      " 30\n",
      "Observer that both results are same.\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(6.0) # constant Tensor\n",
    "b = tf.constant(5.0)\n",
    "c_manual = a*b\n",
    "c_function = tf.multiply(a,b) # Operation\n",
    "\n",
    "with tf.Session() as s: # Launching Session\n",
    "    \n",
    "    output = s.run(c_manual) # Running Session\n",
    "    print(\"Without using inbuilt function output is:\\n %d\"%output)\n",
    "    \n",
    "    output = s.run(c_function)\n",
    "    print(\"With using inbuilt function output is:\\n %d\"%output)\n",
    "    \n",
    "    print(\"Observer that both results are same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builigin a simple counter using TensorFlow computational Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "blank = tf.Variable(0)\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(blank,one)\n",
    "update = tf.assign(blank,new_value) # Updating Variable\n",
    "\n",
    "init_op = tf.global_variables_initializer() #Intilization of the variables\n",
    "\n",
    "with tf.Session() as session: # Launching Session\n",
    "    session.run(init_op)\n",
    "    print(session.run(blank))\n",
    "    for _ in range(3):\n",
    "        session.run(update)\n",
    "        print(session.run(blank))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables must be initialized by running an initialization operation after having launched the graph.  We first have to add the initialization operation to the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer() #Intilization of the variables\n",
    "with tf.Session() as session: # Launching Session\n",
    "    session.run(init_op)\n",
    "    print(session.run(blank))\n",
    "    for _ in range(3):\n",
    "        session.run(update)\n",
    "        print(session.run(blank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression with TensorFlow directional graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "m = tf.Variable([.3],tf.float32) #from data we can calculate slope.\n",
    "c = tf.Variable([-.3],tf.float32)# from data we can calculate intercept.(when x=0 value of y)\n",
    "#inputs and ouputss\n",
    "X= tf.placeholder(tf.float32) #Manually giving datapoints(Consider as ACtual)\n",
    "init = tf.global_variables_initializer()\n",
    "linear_model = m*X+c\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(linear_model,{X:[1.0,2.0,3.0,4.0]}))\n",
    "    \n",
    "    ### Here We have created a model but how good it is yet we have to compare with actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving Actual Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.3613\n"
     ]
    }
   ],
   "source": [
    "m = tf.Variable(tf.random_normal([1])) #from data we can calculate slope.\n",
    "\n",
    "c = tf.Variable(tf.random_normal([1])) # from data we can calculate intercept.(when x=0 value of y)\n",
    "\n",
    "#inputs and ouputs:\n",
    "\n",
    "X= tf.placeholder(tf.float32)\n",
    "\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "linear_model = m*X+c\n",
    "\n",
    "squared_deltas = tf.square(linear_model-y)\n",
    "\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(loss,{X:[1,2,3,4],y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "## So in order to improve the accuracy of the model we have to change the variables\n",
    "#(In this case W and b)\n",
    "\n",
    "## Let us take W:-1 and b:1\n",
    "\n",
    "m = tf.Variable([-1],tf.int32 ) #imaging variable value updated to -1\n",
    "\n",
    "c = tf.Variable([1],tf.int32 )# imaging variable value updated to 1\n",
    "\n",
    "#inputs and outputs:\n",
    "#===================\n",
    "\n",
    "X= tf.placeholder(tf.int32 )\n",
    "\n",
    "y = tf.placeholder(tf.int32 )\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "linear_model = m*X+c\n",
    "\n",
    "squared_deltas = tf.square(linear_model-y)\n",
    "\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(loss,{X:[1,2,3,4],y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improving accuracy for above model:\n",
    "      For Above model I consider m:-1 and c:1.\n",
    "    # Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function.\n",
    "      Proof is here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "0 9.90946 [ 0.1341247] [-0.9028064]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "20 1.73601 [-0.45171413] [-0.61202735]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "40 1.07216 [-0.569116] [-0.26685154]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "60 0.662165 [-0.66137916] [ 0.00441344]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "80 0.408953 [-0.73388648] [ 0.21759373]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "100 0.252569 [-0.7908681] [ 0.38512671]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "120 0.155986 [-0.83564854] [ 0.51678669]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "140 0.0963369 [-0.87084037] [ 0.62025487]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "160 0.0594976 [-0.89849669] [ 0.70156795]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "180 0.0367457 [-0.92023116] [ 0.76546979]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "200 0.0226941 [-0.93731165] [ 0.81568861]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "220 0.0140159 [-0.95073485] [ 0.8551544]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "240 0.00865616 [-0.96128374] [ 0.88616949]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "260 0.00534603 [-0.96957392] [ 0.91054362]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "280 0.0033017 [-0.97608888] [ 0.92969847]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "300 0.00203914 [-0.9812088] [ 0.94475168]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "320 0.00125938 [-0.98523247] [ 0.95658159]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "340 0.00077779 [-0.98839456] [ 0.96587867]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "360 0.000480363 [-0.9908796] [ 0.97318482]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "380 0.000296671 [-0.99283248] [ 0.97892666]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "400 0.000183223 [-0.99436724] [ 0.98343903]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "420 0.00011316 [-0.99557334] [ 0.98698515]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "440 6.98858e-05 [-0.99652123] [ 0.98977202]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "460 4.31622e-05 [-0.99726611] [ 0.99196208]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "480 2.66563e-05 [-0.99785155] [ 0.99368328]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "500 1.64624e-05 [-0.99831158] [ 0.99503589]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "520 1.01666e-05 [-0.99867314] [ 0.99609888]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "540 6.27889e-06 [-0.99895728] [ 0.99693424]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "560 3.87779e-06 [-0.99918056] [ 0.99759072]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "580 2.39476e-06 [-0.99935603] [ 0.9981066]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "600 1.4791e-06 [-0.9994939] [ 0.99851203]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "620 9.13347e-07 [-0.99960232] [ 0.99883074]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "640 5.64058e-07 [-0.99968743] [ 0.99908113]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "660 3.4835e-07 [-0.99975443] [ 0.99927795]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "680 2.15099e-07 [-0.999807] [ 0.99943256]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "700 1.32828e-07 [-0.99984831] [ 0.99955404]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "720 8.20592e-08 [-0.99988079] [ 0.99964952]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "740 5.06845e-08 [-0.9999063] [ 0.99972457]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "760 3.12939e-08 [-0.99992639] [ 0.99978364]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "780 1.93474e-08 [-0.99994212] [ 0.99982989]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "800 1.19403e-08 [-0.99995458] [ 0.99986637]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "820 7.37946e-09 [-0.99996424] [ 0.99989498]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "840 4.55121e-09 [-0.99997193] [ 0.99991739]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "860 2.81483e-09 [-0.99997795] [ 0.99993509]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "880 1.73877e-09 [-0.99998266] [ 0.99994898]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "900 1.07461e-09 [-0.99998635] [ 0.99995983]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "920 6.61998e-10 [-0.99998927] [ 0.99996853]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "940 4.06672e-10 [-0.9999916] [ 0.99997526]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "960 2.53937e-10 [-0.99999338] [ 0.99998057]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "980 1.55485e-10 [-0.99999481] [ 0.99998474]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1000 9.28821e-11 [-0.99999601] [ 0.99998814]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1020 5.84635e-11 [-0.99999678] [ 0.99999058]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1040 3.53566e-11 [-0.99999756] [ 0.99999279]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1060 2.27516e-11 [-0.99999803] [ 0.9999941]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1080 1.44382e-11 [-0.99999839] [ 0.99999529]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1100 8.24585e-12 [-0.99999881] [ 0.99999648]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1120 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1140 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1160 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1180 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1200 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1220 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1240 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1260 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1280 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1300 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1320 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1340 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1360 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1380 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1400 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1420 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1440 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1460 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1480 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1500 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1520 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1540 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1560 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1580 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1600 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1620 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1640 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1660 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1680 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1700 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1720 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1740 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1760 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1780 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1800 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1820 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1840 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1860 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1880 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1900 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1920 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1940 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1960 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "1980 4.20641e-12 [-0.99999911] [ 0.99999744]\n",
      "step: Loss:\tSlope:\t\tIntercept:\n",
      "2000 4.20641e-12 [-0.99999911] [ 0.99999744]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "m=tf.Variable(tf.random_normal([1]))\n",
    "c=tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "X=[1,2,3,4]\n",
    "y=[0,-1,-2,-3]\n",
    "\n",
    "linear_model = m*X+c\n",
    "\n",
    "##--------------------Loss function calculation---------------------------------##\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(linear_model-y))\n",
    "\n",
    "##------------------Applying Gradient Descent method----------------------------##\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "##----------------------------Launching and Running Session----------------------------##\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer()) #---Intinalizing Varibales------#\n",
    "\n",
    "##----------------Performing Itteration to know loss for each m and c value------##\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step %20==0:\n",
    "        print(\"step: Loss:\\tSlope:\\t\\tIntercept:\")\n",
    "        print(step,sess.run(loss),sess.run(m),sess.run(c)) ##--we have to choose m and c which has min Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:2 Linear Regression by TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00740377 [ 0.26721364] [ 0.28751102]\n",
      "20 0.000104863 [ 0.13549262] [ 0.28094473]\n",
      "40 7.24762e-06 [ 0.10933096] [ 0.29499042]\n",
      "60 5.0093e-07 [ 0.10245311] [ 0.29868299]\n",
      "80 3.46239e-08 [ 0.10064494] [ 0.29965377]\n",
      "100 2.3927e-09 [ 0.10016954] [ 0.299909]\n",
      "120 1.65397e-10 [ 0.10004457] [ 0.29997608]\n",
      "140 1.14408e-11 [ 0.10001173] [ 0.29999372]\n",
      "160 8.02336e-13 [ 0.1000031] [ 0.29999834]\n",
      "180 5.66125e-14 [ 0.10000083] [ 0.29999956]\n",
      "200 3.97904e-15 [ 0.10000021] [ 0.29999989]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "#-------Here I am creating X and Y value using weights(W) and Bias(b) itself-----------##\n",
    "\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    " \n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))# generate one number randomly in_between -1 to 1\n",
    "\n",
    "b = tf.Variable(tf.zeros([1]))#it creates tensor of value 0, [1] represent s\n",
    "y = W * x_data + b\n",
    " \n",
    "##----------------- Minimize the mean squared errors--------------------------##\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "train = optimizer.minimize(loss)\n",
    " \n",
    "#----------- Before starting, initialize the variables.  We will 'run' this first--------------\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "##------------------------ Launch the graph----------------------------##\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    " \n",
    "    \n",
    "    # Here we know by taking W=0.1 and b=0.3 error will be 0. Let TensorFlow figout my W and b values.\n",
    "    # Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step,sess.run(loss),sess.run(W), sess.run(b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
